# Resumen T√©cnico Detallado: Private-GPT Main

Este documento ofrece un an√°lisis profundo de la arquitectura interna de `private-gpt-main`. Este proyecto es una API robusta construida sobre **FastAPI** y **LlamaIndex** que permite la ingesti√≥n de documentos y la generaci√≥n de respuestas aumentadas por recuperaci√≥n (RAG) de manera local y privada.

## üèóÔ∏è Arquitectura de Alto Nivel

El sistema sigue una arquitectura modular basada en **Inyecci√≥n de Dependencias** (usando la librer√≠a `injector`). Esto permite cambiar f√°cilmente entre diferentes implementaciones de LLM (Ollama, OpenAI, LlamaCPP) y bases de datos vectoriales (Qdrant, Chroma, Postgres).

### Componentes Principales

1.  **Server (FastAPI):** Expone endpoints REST compatibles con la API de OpenAI.
2.  **Core (LlamaIndex):** Orquesta la l√≥gica de RAG, embeddings y chat.
3.  **Components:** M√≥dulos intercambiables para LLM, Embeddings y Vector Store.

---

## üìÇ An√°lisis Archivo por Archivo

A continuaci√≥n, se destacan los archivos m√°s cr√≠ticos del sistema.

### 1. Configuraci√≥n y Arranque

#### üìÑ `settings.yaml` (El Cerebro de Configuraci√≥n)
**Importancia:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
Define el comportamiento global del sistema.
*   **`ui.default_query_system_prompt`:** Aqu√≠ reside el prompt maestro que instruye a la IA sobre c√≥mo comportarse (ej: reglas de "has_information", "needs_contact").
*   **`llm` y `ollama`:** Configura el modelo a usar (ej: `llama3.1`), la ventana de contexto y la temperatura.
*   **`vectorstore`:** Define qu√© base de datos usar (actualmente `qdrant`).

#### üìÑ `private_gpt/launcher.py`
**Importancia:** ‚≠ê‚≠ê‚≠ê‚≠ê
Es la f√°brica de la aplicaci√≥n FastAPI.
*   Configura el contenedor de inyecci√≥n de dependencias (`Injector`).
*   Registra los routers (`chat_router`, `ingest_router`, etc.).
*   Configura CORS y monta la UI si est√° habilitada.

### 2. Capa de Servicio (L√≥gica de Negocio)

#### üìÑ `private_gpt/server/chat/chat_router.py`
**Importancia:** ‚≠ê‚≠ê‚≠ê‚≠ê
El punto de entrada para las peticiones de chat (`/v1/chat/completions`).
*   Recibe el JSON del usuario.
*   Delega la l√≥gica al `ChatService`.
*   Maneja el streaming de respuestas (SSE) para que el texto aparezca "escribi√©ndose".

#### üìÑ `private_gpt/server/chat/chat_service.py` (El Orquestador RAG)
**Importancia:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Cr√≠tica)
Aqu√≠ ocurre la magia del RAG.
*   **Clase `ChatService`:** Inicializa el `VectorStoreIndex` de LlamaIndex.
*   **M√©todo `_chat_engine`:**
    *   Si `use_context=True`: Crea un `ContextChatEngine`. Configura el retriever para buscar en la base vectorial y aplica post-procesadores (reranking, filtros).
    *   Si `use_context=False`: Crea un `SimpleChatEngine` (chat normal sin documentos).
*   **Integraci√≥n:** Une el LLM, el modelo de Embeddings y el Vector Store.

#### üìÑ `private_gpt/server/ingest/ingest_service.py`
**Importancia:** ‚≠ê‚≠ê‚≠ê‚≠ê
Responsable de "leer" y "aprender" los documentos.
*   Usa `SentenceWindowNodeParser` para dividir los textos en fragmentos inteligentes.
*   Genera embeddings y los guarda en Qdrant.

### 3. Componentes Modulares

#### üìÑ `private_gpt/components/llm/llm_component.py`
**Importancia:** ‚≠ê‚≠ê‚≠ê
Abstracci√≥n que carga el modelo de lenguaje correcto seg√∫n `settings.yaml`.
*   Soporta `ollama`, `openai`, `llamacpp`, etc.
*   En el caso de `ollama`, configura el cliente y par√°metros como `keep_alive` y `request_timeout`.

#### üìÑ `private_gpt/components/vector_store/vector_store_component.py`
**Importancia:** ‚≠ê‚≠ê‚≠ê
Abstracci√≥n para la base de datos vectorial.
*   Inicializa el cliente de Qdrant (o Chroma/Milvus).
*   Provee el `retriever` que usa el `ChatService` para buscar informaci√≥n relevante.

---

## üîÑ Flujo de una Petici√≥n RAG

1.  **Petici√≥n:** Llega a `chat_router.py` con `use_context=True`.
2.  **Servicio:** `ChatService` recibe el mensaje.
3.  **B√∫squeda:** El `retriever` (de `VectorStoreComponent`) convierte la pregunta en n√∫meros (embeddings) y busca fragmentos similares en Qdrant.
4.  **Contexto:** Los fragmentos encontrados se inyectan en el `System Prompt` (definido en `settings.yaml`).
5.  **Generaci√≥n:** El `LLMComponent` (Ollama) recibe el prompt enriquecido y genera la respuesta.
6.  **Respuesta:** Se env√≠a al usuario v√≠a streaming.
