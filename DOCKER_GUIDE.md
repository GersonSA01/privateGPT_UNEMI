# Gu√≠a de uso de PrivateGPT con Docker - CPU/GPU Switch

Esta gu√≠a te ayudar√° a cambiar f√°cilmente entre modo CPU y GPU para ejecutar PrivateGPT con Ollama.

## üéØ Configuraci√≥n R√°pida

### Opci√≥n 1: Usar scripts helper (Recomendado)

#### Para CPU (desarrollo local):
```powershell
.\start-cpu.ps1
```

#### Para GPU (servidor con NVIDIA):
```powershell
.\start-gpu.ps1
```

#### Para usar el modo configurado en .env:
```powershell
.\start.ps1
```

#### Para detener:
```powershell
.\stop.ps1
```

### Opci√≥n 2: Comandos Docker Compose directos

#### CPU:
```powershell
docker-compose --profile ollama-cpu up
```

#### GPU:
```powershell
docker-compose --profile ollama-cuda up
```

## üìù Configuraci√≥n mediante archivo .env

1. Edita el archivo `.env`:
```powershell
# Para CPU (desarrollo)
OLLAMA_MODE=cpu

# Para GPU (servidor)
OLLAMA_MODE=gpu
```

2. Luego ejecuta:
```powershell
.\start.ps1
```

## üîÑ Cambiar entre CPU y GPU

### M√©todo 1: Editar .env
1. Edita `.env` y cambia `OLLAMA_MODE` a `cpu` o `gpu`
2. Ejecuta `.\start.ps1`

### M√©todo 2: Usar scripts espec√≠ficos
- `.\start-cpu.ps1` - Inicia en modo CPU
- `.\start-gpu.ps1` - Inicia en modo GPU

### M√©todo 3: Especificar modo en l√≠nea de comandos
```powershell
.\start.ps1 -Mode cpu
.\start.ps1 -Mode gpu
```

## üì¶ Requisitos

### Para CPU:
- Docker Desktop instalado
- Al menos 8GB de RAM recomendado

### Para GPU:
- Docker Desktop con soporte NVIDIA Container Toolkit
- GPU NVIDIA con CUDA
- NVIDIA Container Toolkit instalado

## üöÄ Primera ejecuci√≥n

En la primera ejecuci√≥n, Docker:
1. Construir√° las im√°genes necesarias (puede tardar varios minutos)
2. Descargar√° la imagen de Ollama
3. Iniciar√° los servicios

**IMPORTANTE**: Despu√©s de iniciar, necesitas descargar los modelos de Ollama:

```powershell
# Para CPU
docker-compose exec ollama-cpu ollama pull llama3.1-instruct-q4_K_M 
docker-compose exec ollama-cpu ollama pull nomic-embed-text

# Para GPU
docker-compose exec ollama-cuda ollama pull llama3.1-instruct-q4_K_M 
docker-compose exec ollama-cuda ollama pull nomic-embed-text
```

## üåê Acceso a los servicios

Una vez iniciado:
- **PrivateGPT API y UI**: http://localhost:8001
- **Ollama API**: http://localhost:11434

## üîç Ver logs

```powershell
# CPU
docker-compose --profile ollama-cpu logs -f

# GPU
docker-compose --profile ollama-cuda logs -f
```

## üõë Detener servicios

```powershell
# Detener CPU
docker-compose --profile ollama-cpu down

# Detener GPU
docker-compose --profile ollama-cuda down

# O usar el script
.\stop.ps1
```

## üí° Tips

- Los modelos se almacenan en `./models` (compartido entre CPU y GPU)
- Los datos de PrivateGPT est√°n en `./local_data`
- Si cambias entre CPU y GPU, no necesitas descargar los modelos de nuevo
- Usa `docker-compose down -v` si quieres eliminar los vol√∫menes tambi√©n

## ‚ö†Ô∏è Troubleshooting

### Error: "No se puede conectar a Ollama"
- Aseg√∫rate de que el servicio de Ollama est√© corriendo
- Verifica los logs: `docker-compose logs ollama-cpu` o `docker-compose logs ollama-cuda`

### Error con GPU: "Cannot connect to the Docker daemon"
- Aseg√∫rate de tener NVIDIA Container Toolkit instalado
- Verifica: `docker run --rm --gpus all nvidia/cuda:11.0.3-base-ubuntu20.04 nvidia-smi`

### Error: "Port already in use"
- Det√©n otros servicios que usen los puertos 8001 o 11434
- O cambia los puertos en `docker-compose.yaml`

